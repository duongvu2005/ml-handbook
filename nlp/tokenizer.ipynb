{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e32d49f",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47734e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple, Set\n",
    "import string\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdfc9c",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd3612",
   "metadata": {},
   "source": [
    "Before a model can work with text, we need to turn that text into numbers. Tokenization is the first step in that process — it splits text into smaller pieces (tokens) and assigns each one an ID.\n",
    "\n",
    "Depending on the tokenizer, a token can be a whole word, a subword, or even a single character. For example, simple tokenizers might just split on spaces, while more advanced ones (like BPE or WordPiece) learn which parts of words are common and reuse them to handle new words.\n",
    "\n",
    "The goal is to convert text into a form that’s consistent, compact, and meaningful for a model to process. Tokenization might seem simple, but it affects almost everything downstream — vocabulary size, model efficiency, and how well the model handles unseen words.\n",
    "\n",
    "In general, a **tokenizer** is defined as a tuple $(\\Sigma_0, \\Sigma, f)$, where\n",
    "1. $\\Sigma_0$ is a (finite) **base vocabulary** describing the minimal units from which strings are built.\n",
    "2. $\\Sigma$ is a (finite) **token vocabulary** that the tokenizer uses to represent outputs.\n",
    "3. $f: \\Sigma_0^* \\to \\Sigma^*$ is a **tokenization function** that takes strings represented as sequences in the base vocabulary and transforms them into tokens.\n",
    "\n",
    "Here, we define $\\Sigma^*$ as the set of all strings that can be made from a vocabulary $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df33aa",
   "metadata": {},
   "source": [
    "## 2. Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde9a90",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) is a subword tokenization method that balances between word-level and character-level tokenization. It starts with all characters in the text as individual tokens and then repeatedly merges the most frequent pairs of tokens. Over time, this builds up a vocabulary of common subwords.\n",
    "\n",
    "The key idea is:\n",
    "- Frequent character pairs (like t + h → th, then th + e → the) get merged first.\n",
    "- Rare words can still be represented by smaller subword pieces (like un, ##seen, ##ly).\n",
    "\n",
    "This helps models handle unknown or rare words better, while keeping the vocabulary size manageable.\n",
    "\n",
    "In practice, BPE is widely used in modern NLP models (like GPT or RoBERTa). It provides a nice compromise:\n",
    "- **Efficient:** Fewer tokens than character-level.\n",
    "- **Flexible:** Can represent unseen words.\n",
    "- **Consistent:** Keeps common patterns intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecd508",
   "metadata": {},
   "source": [
    "### Algorithm 1: Dictionary learning for byte pair encoding\n",
    "\n",
    "**Input:**  \n",
    "Base vocabulary $\\Sigma_0$, dataset $X$, target vocabulary size $ V $\n",
    "\n",
    "**Output:**  \n",
    "Learned vocabulary $\\Sigma$, merge history $M$\n",
    "\n",
    "\n",
    "**Initialize**  \n",
    "$\\Sigma \\leftarrow \\Sigma_0$  \n",
    "$M \\leftarrow []$\n",
    "\n",
    "**while** $|\\Sigma| < V$ **do**  \n",
    "$\\quad (a^*, b^*) = \\arg\\max_{a,b \\in \\Sigma}\n",
    "\\sum_{\\mathbf{x} \\in X} \\sum_{i=1}^{|\\mathbf{x}|-1}\n",
    "\\mathbf{1}[x_i = a \\land x_{i+1} = b]$  \n",
    "$\\quad \\text{new\\_tok} \\leftarrow a^* b^*$  \n",
    "$\\quad \\Sigma \\leftarrow \\Sigma \\cup \\{\\text{new\\_tok}\\}$  \n",
    "$\\quad M \\leftarrow M + [(\\text{new\\_tok}, a^*, b^*)]$  \n",
    "$\\quad$**for each** $\\mathbf{x} \\in X$ **do**  \n",
    "$\\qquad \\mathbf{x} \\leftarrow \\text{replace}([a^*, b^*], [\\text{new\\_tok}], \\mathbf{x}) $  \n",
    "$\\quad$**end for**  \n",
    "**end while**\n",
    "\n",
    "**return** $ \\Sigma, M $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a6156",
   "metadata": {},
   "source": [
    "To implement this algorithm, we first write some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf1695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pair_frequencies(X: List[List[str]]) -> Counter:\n",
    "\t\"\"\"\n",
    "\tGiven a dataset X, returns a counter that count how often each adjacent\n",
    "\tpair of symbols appear in the dataset.\n",
    "\t\"\"\"\n",
    "\tpair_freq = Counter()\n",
    "\tfor x in X:\n",
    "\t\tfor idx in range(len(x) - 1):\n",
    "\t\t\tpair_freq[(x[idx], x[idx+1])] += 1\n",
    "\n",
    "\treturn pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d7f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(X: List[List[str]], a: str, b: str, new_tok: str) -> List[List[str]]:\n",
    "\t\"\"\"\n",
    "\tGiven a dataset X, a pair of token (a, b), and the new token new_tok,\n",
    "\treplace every occurrence of the pair (a, b) with new_tok in the dataset.\n",
    "\t\"\"\"\n",
    "\tnew_X = []\n",
    "\tfor x in X:\n",
    "\t\tnew_seq = []\n",
    "\t\tidx = 0\n",
    "\t\twhile idx < len(x):\n",
    "\t\t\tif idx < len(x) - 1 and x[idx] == a and x[idx+1] == b:\n",
    "\t\t\t\tnew_seq.append(new_tok)\n",
    "\t\t\t\tidx += 2\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_seq.append(x[idx])\n",
    "\t\t\t\tidx += 1\n",
    "\t\tnew_X.append(new_seq)\n",
    "\n",
    "\treturn new_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b980a1e",
   "metadata": {},
   "source": [
    "Now, we are ready to write the main function for our learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8535491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bpe(\n",
    "\tX: List[List[str]],\n",
    "\ttarget_vocab_size: int,\n",
    "\tbase_vocab: Set[str] = None,\n",
    "\tverbose=False\n",
    ") -> Tuple[Set[str], List[Tuple[str, str, str]]]:\n",
    "\t\"\"\"\n",
    "\tDictionary learning for Byte Pair Encoding (BPE).\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tX: dataset, list of lists of symbols (each x in X is a sequence of chars)\n",
    "\t\ttarget_vocab_size: desired vocabulary size\n",
    "\t\tbase_vocab: optional initial vocabulary\n",
    "\t\tverbose: whether to print the merges\n",
    "\n",
    "\tReturns:\n",
    "\t\tSigma: final vocabulary\n",
    "\t\tM: list of merges (new_token, a, b)\n",
    "\t\"\"\"\n",
    "\t# initialize base vocab\n",
    "\tif base_vocab is not None:\n",
    "\t\tSigma = base_vocab\n",
    "\telse:\n",
    "\t\tSigma = set(char for x in X for char in x)\n",
    "\t\n",
    "\tM = []\n",
    "\n",
    "\twhile len(Sigma) < target_vocab_size:\n",
    "\t\t# get the most common pair\n",
    "\t\tpair_freq = count_pair_frequencies(X)\n",
    "\n",
    "\t\ta, b = pair_freq.most_common(1)[0][0]\n",
    "\t\t# define new token and merge\n",
    "\t\tnew_tok = a + b\n",
    "\t\tX = replace_pair(X, a, b, new_tok)\n",
    "\t\t# update\n",
    "\t\tSigma.add(new_tok)\n",
    "\t\tM.append((new_tok, a, b))\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"Merge {len(M)}: ({a}, {b}) -> {new_tok}\")\n",
    "\n",
    "\treturn Sigma, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b029926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple basic vocab\n",
    "def generate_basic_vocab(include_special: bool=True) -> Set:\n",
    "\tbase_vocab = list(\n",
    "\t\tstring.ascii_letters +\n",
    "\t\tstring.digits +\n",
    "\t\tstring.punctuation +\n",
    "\t\t' '\n",
    "\t)\n",
    "\tif include_special:\n",
    "\t\tbase_vocab += ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
    "\n",
    "\treturn set(base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca585e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "base_vocab = generate_basic_vocab()\n",
    "print(len(base_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5827ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Vocabulary Size: 111\n",
      "Learned merges:\n",
      "t + h -> th\n",
      "th + e -> the\n",
      "the +   -> the \n",
      "a + t -> at\n",
      "at +   -> at \n",
      "  + the  ->  the \n",
      "o + g -> og\n",
      "d + og -> dog\n",
      "the  + c -> the c\n",
      "the c + at  -> the cat \n",
      "s + at  -> sat \n",
      "sat  + o -> sat o\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "X = [\n",
    "    list(\"the cat sat on the mat\"),\n",
    "    list(\"the dog sat on the log\"),\n",
    "    list(\"the cat chased the dog\"),\n",
    "    list(\"the dog chased the cat\"),\n",
    "]\n",
    "\n",
    "Sigma, M = learn_bpe(X, target_vocab_size=111, base_vocab=base_vocab)\n",
    "\n",
    "print(\"\\nFinal Vocabulary Size:\", len(Sigma))\n",
    "print(\"Learned merges:\")\n",
    "for new_tok, a, b in M:\n",
    "    print(f\"{a} + {b} -> {new_tok}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23369618",
   "metadata": {},
   "source": [
    "### Algorithm 2: Tokenization for byte pair encoding\n",
    "**Input:**  \n",
    "Merge history $M$, string $\\mathbf{x}$\n",
    "\n",
    "**Output**  \n",
    "Tokenized $\\mathbf{x}$\n",
    "\n",
    "**for each** $\\text{tok}, a, b \\in \\Sigma$ **do**  \n",
    "$\\quad$ $\\mathbf{x} \\leftarrow \\text{replace}([a, b], \\text{tok}, \\mathbf{x})$  \n",
    "**end for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31690857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x, merge_history):\n",
    "\tX = [x]\n",
    "\tfor tok, a, b in merge_history:\n",
    "\t\tX = replace_pair(X, a, b, tok)\n",
    "\n",
    "\treturn X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7976945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'e', ' ', 'c', 'at ', 'at', 'e', ' the ', 'f', 'i', 's', 'h']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "example_seq = list(\"The cat ate the fish\")\n",
    "tokenized_seq = tokenize(example_seq, M)\n",
    "tokenized_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f408e09",
   "metadata": {},
   "source": [
    "## 3. Using Existing Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e05fbc",
   "metadata": {},
   "source": [
    "Example usage of a tokenizer from Hugging Face's transformers.AutoTokenizer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d652ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocab size: 50257\n",
      "Special tokens: PAD=50256, EOS=50256\n",
      "Tokenizer output has keys: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: PAD={tokenizer.pad_token_id}, EOS={tokenizer.eos_token_id}\")\n",
    "print(f\"Tokenizer output has keys: {list(tokenizer(\"\").keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed32f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: [[51], [71], [68], [220], [66], [64], [83], [220], [64], [83], [68], [220], [83], [71], [68], [220], [69], [72], [82], [71]]\n",
      "Attention mask: [[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input ids: {tokenizer(example_seq)['input_ids']}\")\n",
    "print(f\"Attention mask: {tokenizer(example_seq)['attention_mask']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
