{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5f19e5",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaee3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2af7410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f619c",
   "metadata": {},
   "source": [
    "## 1. Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8ce59",
   "metadata": {},
   "source": [
    "### 1.1 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a71026",
   "metadata": {},
   "source": [
    "[WIP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ecc554",
   "metadata": {},
   "source": [
    "### 1.2 Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89066762",
   "metadata": {},
   "source": [
    "#### 1.2.1 Token by token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda17597",
   "metadata": {},
   "source": [
    "For each token, we embed them using a triplet of query, key, and value vectors. Keep in mind that\n",
    "- The **query** is the **question.** Given a token and its query vector, we use **that query vector** and compares it with other tokens to **ask** \"how much attention should we pay to other tokens?\"\n",
    "- The **key** is the **identity.** When our token (let's call it A) is queried by another token B, we use **A's key** and **B's query** to calculate the attention.\n",
    "- The **value** is the **contribution.** After the attention has been calculated, the **contribution** to the final output of our token is the attention times **its value vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd507c23",
   "metadata": {},
   "source": [
    "To embed our tokens as described, we will need 3 learnable weight matrices: one for the query, one for the key, and one for the value. Often, they are call the QKV matrices. The dimensions of them are\n",
    "$$\n",
    "W_q \\in \\mathbb{R}^{d \\times d_k}, \\quad\n",
    "W_k \\in \\mathbb{R}^{d \\times d_k}, \\quad\n",
    "W_v \\in \\mathbb{R}^{d \\times d_k}.\n",
    "$$\n",
    "Then, for each token $x_i$, we compute the three QKV vectors\n",
    "$$\n",
    "q_i = W_q^T x_i, \\quad\n",
    "k_i = W_k^T x_i, \\quad\n",
    "v_i = W_v^T x_i.\n",
    "$$\n",
    "The dot product between the query and the key tells us how \"similar\" these vectors are. Given a query, the attention mechanism works by computing the dot product of that query with all of the keys, and then applies a softmax function to get the probability distribution over the keys\n",
    "$$\n",
    "a_i = p(k | q_i) = \\text{softmax} \\left(\n",
    "\t\\frac{[q_i^T k_1, q^T_i k_2, \\dots, q^T_i k_n]}{\\sqrt{d_k}}\n",
    "\\right) \\in \\mathbb{R}^{1 \\times n}.\n",
    "$$\n",
    "Here, the factor of $1/\\sqrt{d_k}$ helps with numerical stability (they keep the dot products from getting too large). Given this, the final output is given by\n",
    "$$\n",
    "z_i = \\sum_j p(k_j | q_i) v_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780444c",
   "metadata": {},
   "source": [
    "#### 1.2.2 Doing it all at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50161bf",
   "metadata": {},
   "source": [
    "We start by stacking our inputs into a single matrix $X$\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\tx_1^T  \\\\\n",
    "\tx_2^T  \\\\\n",
    "\t\\vdots \\\\\n",
    "\tx_n^T  \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times d}.\n",
    "$$\n",
    "Then, we can stack the QKV vectors into $Q, K, V$ defined as\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "\tq_1^T  \\\\\n",
    "\tq_2^T  \\\\\n",
    "\t\\vdots \\\\\n",
    "\tq_n^T  \\\\\n",
    "\\end{bmatrix}\n",
    "= XW_q \\in \\mathbb{R}^{n \\times d_k}, \\quad\n",
    "K = \\begin{bmatrix}\n",
    "\tk_1^T  \\\\\n",
    "\tk_2^T  \\\\\n",
    "\t\\vdots \\\\\n",
    "\tk_n^T  \\\\\n",
    "\\end{bmatrix}\n",
    "= XW_k \\in \\mathbb{R}^{n \\times d_k}, \\quad\n",
    "V = \\begin{bmatrix}\n",
    "\tv_1^T  \\\\\n",
    "\tv_2^T  \\\\\n",
    "\t\\vdots \\\\\n",
    "\tv_n^T  \\\\\n",
    "\\end{bmatrix}\n",
    "= XW_q \\in \\mathbb{R}^{n \\times d_k}.\n",
    "$$\n",
    "Note again that **row $i$ in the QKV matrices store the QKV vectors of token $i$**. The attention matrix is thus given by\n",
    "$$\n",
    "A = \\text{softmax} \\left(\n",
    "\t\\frac{1}{\\sqrt{d_k}}\n",
    "\t\\begin{bmatrix}\n",
    "\t\tq_1^T k_1 & q^T_1 k_2 & \\dots  & q^T_1 k_n \\\\\n",
    "\t\tq_2^T k_1 & q^T_2 k_2 & \\dots  & q^T_2 k_n \\\\\n",
    "\t\t\\vdots\t  & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "\t\tq_n^T k_1 & q^T_n k_2 & \\dots & q^T_n k_n  \\\\\n",
    "\t\\end{bmatrix}\n",
    "\\right) = \\text{softmax} \\left(\n",
    "\t\\frac{QK^T}{\\sqrt{d_k}}\n",
    "\\right) \\in \\mathbb{R}^{n \\times n}.\n",
    "$$\n",
    "Here, softmax is applied in a row-wise manner. Row $i$ of this matrix contains the weights of value vectors that we would need to construct the $i$-th attention outputs. It follows that the full output is given by\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "\tz_1^T  \\\\\n",
    "\tz_2^T  \\\\\n",
    "\t\\vdots \\\\\n",
    "\tz_n^T  \\\\\n",
    "\\end{bmatrix}\n",
    "= AV \\in \\mathbb{R}^{n \\times d_k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad06d1",
   "metadata": {},
   "source": [
    "#### 1.2.3 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd4fe2",
   "metadata": {},
   "source": [
    "We will implement a simple attention layer as described above, with a fully connected layer at the end to map our outputs back to the original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\t\"\"\"\n",
    "\tImplementation of a simple attention layer.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_dim, key_dim):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- embed_dim: dimension of the token embedding\n",
    "\t\t- key_dim:   dimension of the key (value, and query)\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_embed = embed_dim\n",
    "\t\tself.d_key   = key_dim\n",
    "\n",
    "\t\t# The QKV matrices\n",
    "\t\tself.Wq = nn.Linear(embed_dim, key_dim)\n",
    "\t\tself.Wk = nn.Linear(embed_dim, key_dim)\n",
    "\t\tself.Wv = nn.Linear(embed_dim, key_dim)\n",
    "\n",
    "\t\t# Fully connected layer at the end\n",
    "\t\tself.Wc = nn.Linear(key_dim, embed_dim)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tInput:\n",
    "\t\t- x: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "\t\tOutput:\n",
    "\t\t- x: (batch_size, seq_len, embed_dim)\n",
    "\t\t- A: the attention matrices (batch_size, seq_len, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\t# Note: x has shape (B, n, d)\n",
    "\t\tQ = self.Wq(x)  # (B, n, d_k)\n",
    "\t\tK = self.Wk(x)  # (B, n, d_k)\n",
    "\t\tV = self.Wv(x)  # (B, n, d_k)\n",
    "\n",
    "\t\tA = F.softmax(Q @ K.transpose(-2, -1) / self.d_key**0.5, dim=-1)  # (b, n, n)\n",
    "\n",
    "\t\t# Forward pass\n",
    "\t\tx = A @ V       # (B, n, d_k)\n",
    "\t\tx = self.Wc(x)  # (B, n, d)\n",
    "\n",
    "\t\treturn x, A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f66e3",
   "metadata": {},
   "source": [
    "We can do a quick test to check that the shapes works out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "757a1acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our input is (10, 20, 4)\n",
      "The shape of our output is (10, 20, 4)\n",
      "The shape of our attention matrices is (10, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "# Setting up the layer\n",
    "embed_dim = 4\n",
    "key_dim = 2\n",
    "\n",
    "attention_layer = Attention(embed_dim, key_dim)\n",
    "attention_layer.to(DEVICE)\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 10\n",
    "seq_len = 20\n",
    "x = torch.randn(batch_size, seq_len, embed_dim, device=DEVICE)\n",
    "print(f\"The shape of our input is {tuple(x.cpu().shape)}\")\n",
    "\n",
    "# Forward pass\n",
    "x, A = attention_layer(x)\n",
    "print(f\"The shape of our output is {tuple(x.cpu().shape)}\")\n",
    "print(f\"The shape of our attention matrices is {tuple(A.cpu().shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846dd9c",
   "metadata": {},
   "source": [
    "[WIP]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
