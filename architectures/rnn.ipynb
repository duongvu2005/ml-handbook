{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545f1edb",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93eba634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfd9c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39be92",
   "metadata": {},
   "source": [
    "## 1. Sequential Data and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c12c8",
   "metadata": {},
   "source": [
    "### 1.1 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d6f76",
   "metadata": {},
   "source": [
    "Sequential data, such as text, audio, or time series, has an inherent **temporal order**, where each element depends on previous ones. Traditional models like **MLPs** and **CNNs** process inputs independently or in fixed-size local windows, so they struggle to capture these **long-term dependencies** and **contextual relationships**. To address this, **Recurrent Neural Networks (RNNs)** were introduced - models specifically designed to handle sequential data by maintaining a **hidden state** that carries information from past time steps, allowing them to learn temporal patterns and dependencies over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4900dd9",
   "metadata": {},
   "source": [
    "### 1.2 RNN Cell mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ee6eb",
   "metadata": {},
   "source": [
    "The RNN model works by storing a hidden state $\\mathbf{h}_t \\in \\mathbb{R}^d$. Its value depends on the hidden state at the previous time step and the input data. That is,\n",
    "$$\n",
    "\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{x}_t).\n",
    "$$\n",
    "The output of the model is then determined by the hidden state\n",
    "$$\n",
    "\\mathbf{y}_t = g(\\mathbf{h}_t).\n",
    "$$\n",
    "We thus have three matrices\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\mathbf{W}_{hh}:  & \\text{ Update the hidden state using the previous hidden state.}\\\\\n",
    "\t\\mathbf{W}_{hx}:  & \\text{ Update the hidden state using the input.}\\\\\n",
    "\t\\mathbf{W}_{yh}:  & \\text{ Calculate output using the hidden state.} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Then, depending on the choice of the activation function, we can write (adding the biases)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\mathbf{h}_t &= \\tanh(\n",
    "\t\t\\mathbf{W}_{hh} \\mathbf{h}_{t-1} +\n",
    "\t\t\\mathbf{W}_{hx} \\mathbf{x}_t +\n",
    "\t\t\\mathbf{b}_h\n",
    "\t), \\\\\n",
    "\t\\mathbf{y}_t &= \\sigma(\n",
    "\t\t\\mathbf{W}_{yh} \\mathbf{h}_t +\n",
    "\t\t\\mathbf{b}_y\n",
    "\t).\n",
    "\\end{aligned}\n",
    "$$\n",
    "However, in the implementation, the linear layer already have the bias in them, so we don't need to explicitly write it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d6893",
   "metadata": {},
   "source": [
    "### 1.3 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b2d958",
   "metadata": {},
   "source": [
    "We separate the implementation of the cell and the model. Given an input and the hidden state, the cell will update that hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "169d9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "\t\"\"\"\n",
    "\tRNN Cell update rule.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_dim, hidden_dim):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- input_dim:  dimension of the input\n",
    "\t\t- hidden_dim: dimention of the hidden state\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = input_dim\n",
    "\t\tself.d_hidden = hidden_dim\n",
    "\n",
    "\t\t# Initialize the matrices\n",
    "\t\tself.Whh = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.Whx = nn.Linear(input_dim,  hidden_dim)\n",
    "\n",
    "\tdef forward(self, x_t, h_prev):\n",
    "\t\t\"\"\"\n",
    "\t\tInput:\n",
    "\t\t- x_t: input at the current time step        (batch_size, input_dim)\n",
    "\t\t- h_prev: hidden state at previous time step (batch_size, hidden_dim)\n",
    "\t\n",
    "\t\tReturns the updated hidden state (batch_size, hidden_dim).\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.tanh(self.Whh(h_prev) + self.Whx(x_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tRNN model\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_dim, output_dim, hidden_dim):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- input_dim:  dimension of the input\n",
    "\t\t- output_dim: dimension of the output\n",
    "\t\t- hidden_dim: dimention of the hidden state\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = input_dim\n",
    "\t\tself.d_output = output_dim\n",
    "\t\tself.d_hidden = hidden_dim\n",
    "\n",
    "\t\tself.RNNCell = RNNCell(input_dim, hidden_dim)\n",
    "\t\tself.Wyh = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\tdef forward(self, x, h_0=None):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- x: input (batch_size, T, input_dim)\n",
    "\t\t- h_0 (optional): provide the current hidden state (batch_size, hidden_dim)\n",
    "\n",
    "\t\tOutput:\n",
    "\t\t- y_T: output (batch_size, T, output_dim)\n",
    "\t\t- h_T: final hidden state (batch_size, hidden_dim)\n",
    "\t\t\"\"\"\n",
    "\t\t# shapes and device\n",
    "\t\tbatch_size, T, _ = x.shape\n",
    "\t\tdevice = x.device\n",
    "\n",
    "\t\t# initializing h_0\n",
    "\t\tif h_0 is None:\n",
    "\t\t\th_t = torch.zeros(batch_size, self.d_hidden, device=device)\n",
    "\t\telse:\n",
    "\t\t\th_t = h_0\n",
    "\n",
    "\t\ty_out = []\n",
    "\t\tfor t in range(T):\n",
    "\t\t\tx_t = x[:, t, :]\n",
    "\t\t\t# update using RNNCell\n",
    "\t\t\th_t = self.RNNCell(x_t, h_t)\n",
    "\t\t\ty_t = self.Wyh(h_t)\n",
    "\t\t\ty_out.append(y_t)\n",
    "\n",
    "\t\treturn torch.stack(y_out, dim=1), h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3079014",
   "metadata": {},
   "source": [
    "We can do a quick test to check that the shapes works out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74192059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:     torch.Size([4, 10, 8]), device: mps:0\n",
      "Output shape:    torch.Size([4, 10, 3]), device: mps:0\n",
      "Hidden state shape: torch.Size([4, 16]), device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "input_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 3\n",
    "\n",
    "# Create random input on your global device\n",
    "x = torch.randn(batch_size, seq_len, input_dim, device=DEVICE)\n",
    "\n",
    "# Initialize and move model to the same device\n",
    "model = RNN(input_dim, output_dim, hidden_dim).to(DEVICE)\n",
    "\n",
    "# Forward pass\n",
    "y_out, h_T = model(x)\n",
    "\n",
    "# Print results\n",
    "print(f\"Input shape:     {x.shape}, device: {x.device}\")\n",
    "print(f\"Output shape:    {y_out.shape}, device: {y_out.device}\")\n",
    "print(f\"Hidden state shape: {h_T.shape}, device: {h_T.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eec60c",
   "metadata": {},
   "source": [
    "## 2. Gated recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a965d9b",
   "metadata": {},
   "source": [
    "RNNs struggle to “remember” information when dependencies in a sequence are separated by many time steps. During training, the gradients that carry learning signals through time can **vanish or explode**, making it difficult for the network to update weights related to long-term dependencies. As a result, standard RNNs tend to forget earlier context as sequences grow longer. **Gated recurrent networks**, such as **LSTMs** and **GRUs**, address this issue by introducing **gates** that regulate how information is stored, forgotten, and passed forward through time. These gates allow the network to preserve important information over long distances, effectively mitigating the vanishing gradient problem and enabling the model to learn long-term temporal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc5964",
   "metadata": {},
   "source": [
    "### 2.1 Long-Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f230b7",
   "metadata": {},
   "source": [
    "#### 2.1.1 LSTM Cell mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f4c04",
   "metadata": {},
   "source": [
    "Similar to a RNN Cell, a LSTM Cell also has a hidden state $\\mathbf{h}_t \\in \\mathbb{R}^d$. In addition to that, it also has a cell state $\\mathbf{C}_t \\in \\mathbb{R}^d$, which acts as a kind of long-term memory that runs through the entire sequence with only minor linear interactions. It allows information to flow across many time steps **without being repeatedly multiplied by weights**, which helps prevent the **vanishing gradient problem** that plagues standard RNNs. There are three gates in a LSTM: two for updating the cell state, and one for updating the hidden state. They are\n",
    "- **forget gate** ($f_t$): how much to *forget* the previous cell state\n",
    "- **input gate** ($i_t$): how much a new value *input* to the current cell state\n",
    "- **output gate** ($o_t$): how much the updated cell state *output* to the hidden state\n",
    "\n",
    "The update rules are as follows\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tf_t &= \\sigma(\n",
    "\t\t\\mathbf{W}_{fx} \\mathbf{x}_t +\n",
    "\t\t\\mathbf{W}_{fh} \\mathbf{h}_{t-1} +\n",
    "\t\t\\mathbf{b}_f\n",
    "\t) \\\\\n",
    "\ti_t &= \\sigma(\n",
    "\t\t\\mathbf{W}_{ix} \\mathbf{x}_t +\n",
    "\t\t\\mathbf{W}_{ih} \\mathbf{h}_{t-1} +\n",
    "\t\t\\mathbf{b}_i\n",
    "\t) \\\\\n",
    "\to_t &= \\sigma(\n",
    "\t\t\\mathbf{W}_{ox} \\mathbf{x}_t +\n",
    "\t\t\\mathbf{W}_{oh} \\mathbf{h}_{t-1} +\n",
    "\t\t\\mathbf{b}_o\n",
    "\t) \\\\\n",
    "\t\\tilde{\\mathbf{C}}_t &= \\tanh(\n",
    "\t\t\\mathbf{W}_{cx} \\mathbf{x}_t +\n",
    "\t\t\\mathbf{W}_{ch} \\mathbf{h}_{t-1} +\n",
    "\t\t\\mathbf{b}_c\n",
    "\t) \\\\\n",
    "\t\\mathbf{C}_t &= f_t \\odot \\mathbf{C}_{t-1} + i_t \\odot \\tilde{\\mathbf{C}}_t \\\\\n",
    "\t\\mathbf{h}_t &= o_t \\odot \\mathbf{C}_t\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1768674",
   "metadata": {},
   "source": [
    "#### 2.1.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73f5fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "\t\"\"\"\n",
    "\tLSTM Cell update rule.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_dim, hidden_dim):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- input_dim:  dimension of the input\n",
    "\t\t- hidden_dim: dimention of the hidden state\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = input_dim\n",
    "\t\tself.d_hidden = hidden_dim\n",
    "\t\t\n",
    "\t\t# Initialize the matrices\n",
    "\t\tself.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "\t\tself.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "\t\tself.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "\t\tself.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.Wcx = nn.Linear(input_dim, hidden_dim)\n",
    "\t\tself.Wch = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\n",
    "\tdef forward(self, x_t, c_prev, h_prev):\n",
    "\t\t\"\"\"\n",
    "\t\tInput:\n",
    "\t\t- x_t: input at the current time step        (batch_size,  input_dim)\n",
    "\t\t- c_prev: cell state at previous time step   (batch_size, hidden_dim)\n",
    "\t\t- h_prev: hidden state at previous time step (batch_size, hidden_dim)\n",
    "\n",
    "\t\tReturns\n",
    "\t\t- c_t: updated cell state   (batch_size, hidden_dim)\n",
    "\t\t- h_t: updated hidden state (batch_size, hidden_dim)\n",
    "\t\t\"\"\"\n",
    "\t\t# calculate the gates\n",
    "\t\tf = torch.sigmoid(self.Wfx(x_t) + self.Wfh(h_prev))\n",
    "\t\ti = torch.sigmoid(self.Wix(x_t) + self.Wih(h_prev))\n",
    "\t\to = torch.sigmoid(self.Wox(x_t) + self.Woh(h_prev))\n",
    "\n",
    "\t\t# new (proposed) value\n",
    "\t\tc_tilde = torch.tanh(self.Wcx(x_t) + self.Wch(h_prev))\n",
    "\n",
    "\t\t# updated cell value\n",
    "\t\tc_t = f * c_prev + i * c_tilde\n",
    "\n",
    "\t\t# updated hidden state\n",
    "\t\th_t = o * torch.tanh(c_t)\n",
    "\n",
    "\t\treturn c_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00444707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\t\"\"\"\n",
    "\tLSTM model\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_dim, output_dim, hidden_dim):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- input_dim:  dimension of the input\n",
    "\t\t- output_dim: dimension of the output\n",
    "\t\t- hidden_dim: dimention of the hidden state\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = input_dim\n",
    "\t\tself.d_output = output_dim\n",
    "\t\tself.d_hidden = hidden_dim\n",
    "\n",
    "\t\tself.LSTMCell = LSTMCell(input_dim, hidden_dim)\n",
    "\t\tself.Wyh = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\tdef forward(self, x, c_0=None, h_0=None):\n",
    "\t\t\"\"\"\n",
    "\t\tInputs:\n",
    "\t\t- x: input (batch_size, T, input_dim)\n",
    "\t\t- c_0 (optional): provide the current cell state (batch_size, hidden_dim)\n",
    "\t\t- h_0 (optional): provide the current hidden state (batch_size, hidden_dim)\n",
    "\n",
    "\t\tOutput:\n",
    "\t\t- y_T: output (batch_size, T, output_dim)\n",
    "\t\t- c_T: final cell state (batch_size, hidden_dim)\n",
    "\t\t- h_T: final hidden state (batch_size, hidden_dim)\n",
    "\t\t\"\"\"\n",
    "\t\t# shapes and device\n",
    "\t\tbatch_size, T, _ = x.shape\n",
    "\t\tdevice = x.device\n",
    "\n",
    "\t\t# initializing the cell and hidden states\n",
    "\t\tif c_0 is None:\n",
    "\t\t\tc_t = torch.zeros(batch_size, self.d_hidden, device=device)\n",
    "\t\telse:\n",
    "\t\t\tc_t = c_0\n",
    "\n",
    "\t\tif h_0 is None:\n",
    "\t\t\th_t = torch.zeros(batch_size, self.d_hidden, device=device)\n",
    "\t\telse:\n",
    "\t\t\th_t = h_0\n",
    "\n",
    "\t\t# update\n",
    "\t\ty_out = []\n",
    "\t\tfor t in range(T):\n",
    "\t\t\tx_t = x[:, t, :]\n",
    "\t\t\t# update using LSTMCell\n",
    "\t\t\tc_t, h_t = self.LSTMCell(x_t, c_t, h_t)\n",
    "\t\t\ty_t = self.Wyh(h_t)\n",
    "\t\t\ty_out.append(y_t)\n",
    "\n",
    "\t\treturn torch.stack(y_out, dim=1), c_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4d8d1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:   torch.Size([4, 10, 8]), device: mps:0\n",
      "Output shape:  torch.Size([4, 10, 3]), device: mps:0\n",
      "Cell state:    torch.Size([4, 16]),    device: mps:0\n",
      "Hidden state:  torch.Size([4, 16]),    device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "input_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 3\n",
    "\n",
    "# Random input on the correct device\n",
    "x = torch.randn(batch_size, seq_len, input_dim, device=DEVICE)\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = LSTM(input_dim, output_dim, hidden_dim).to(DEVICE)\n",
    "\n",
    "# Forward pass (without providing initial states)\n",
    "y_out, c_T, h_T = model(x)\n",
    "\n",
    "# Print shapes and device info\n",
    "print(f\"Input shape:   {x.shape}, device: {x.device}\")\n",
    "print(f\"Output shape:  {y_out.shape}, device: {y_out.device}\")\n",
    "print(f\"Cell state:    {c_T.shape},    device: {c_T.device}\")\n",
    "print(f\"Hidden state:  {h_T.shape},    device: {h_T.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015a97a",
   "metadata": {},
   "source": [
    "### 2.2 Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12ad6b",
   "metadata": {},
   "source": [
    "### 2.2.1 GRU Cell mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e041a15",
   "metadata": {},
   "source": [
    "[WIP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce3704",
   "metadata": {},
   "source": [
    "#### 2.2.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7ec30",
   "metadata": {},
   "source": [
    "[WIP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd75924",
   "metadata": {},
   "source": [
    "## 3. Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca3e65",
   "metadata": {},
   "source": [
    "[WIP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d293d6",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73944a",
   "metadata": {},
   "source": [
    "[WIP]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
